{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f0070-5647-4533-828d-fcfdd215635b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "433d883c-4c15-445f-8bc4-c93ab14ad772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys, re, json\n",
    "from konlpy.tag import Okt \n",
    "from collections import Counter \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load raw data from csv file\n",
    "source=pd.read_csv('반려동물_5.csv')\n",
    "\n",
    "# load stop words\n",
    "stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()\n",
    "\n",
    "#load senti words\n",
    "sentiwordsfile = open('SentiWord_info.json', encoding='utf-8-sig', mode='r')\n",
    "sentiwords = json.load(sentiwordsfile)\n",
    "\n",
    "# 명사 형태소 추출 함수\n",
    "okt = Okt()  \n",
    "\n",
    "#define functions\n",
    "def applyRegularExpression(text):\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 한글 추출 규칙: 띄어 쓰기(1 개)를 포함한 한글\n",
    "    result = hangul.sub('', text)  # 위에 설정한 \"hangul\"규칙을 \"text\"에 적용(.sub)시킴\n",
    "    return result\n",
    "\n",
    "def countWords(_data, range):\n",
    "    if range is None:\n",
    "        range = len(_data)\n",
    "    \n",
    "    data = _data['text'].dropna(axis=0)\n",
    "\n",
    "    corpus = \"\".join(data)\n",
    "    # print('띄어쓰기 없앰', corpus)\n",
    "\n",
    "    # 문장부호 없애기\n",
    "    nouns = okt.nouns(applyRegularExpression(corpus))\n",
    "    #print('문장부호 없앰', nouns)\n",
    "\n",
    "    no_stop_words = [x for x in nouns if x not in stopwords if len(x) > 1]\n",
    "    result = Counter(no_stop_words[0:range])\n",
    "    return result\n",
    "\n",
    "def countWordsByReview(reviews, range):\n",
    "    if range is None:\n",
    "        range = len(reviews)\n",
    "    for row in data[0:range]:\n",
    "        result = Counter(row)\n",
    "    return result\n",
    "\n",
    "def getScoresFromReviews( rawData, range = None ):\n",
    "    if range is None:\n",
    "        range = len(rawData)\n",
    "        \n",
    "    # print(rawData[0:10])\n",
    "    # 리뷰 없는 ㅇㅕㄹ 제거\n",
    "    data = rawData[0:range].dropna(axis=0)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    # 리뷰별로 로직 적용\n",
    "    for row in data['text'][0:range]:\n",
    "        # row 는 리뷰 하나다\n",
    "        # 띄어쓰기 없애기\n",
    "        corpus = \"\".join(row)\n",
    "        # print('띄어쓰기 없앰', corpus)\n",
    "\n",
    "        # 문장부호 없애기\n",
    "        nouns = okt.nouns(applyRegularExpression(corpus))\n",
    "        #print('문장부호 없앰', nouns)\n",
    "\n",
    "        no_stop_words = [x for x in nouns if x not in stopwords if len(x) > 1]\n",
    "        #print('중성명사 없앰', no_stop_words)\n",
    "\n",
    "        #리뷰별로 점수 초기화\n",
    "        score = 0\n",
    "\n",
    "        for word in no_stop_words:\n",
    "            #각 단어의 점수 계산\n",
    "            wordscore = getScoreByWord(word)\n",
    "            # 리뷰 내 모든 단어의 점수 합산\n",
    "            score += int(getScoreByWord(word))\n",
    "        \n",
    "        # add values to scores array(new column value)\n",
    "        scores.append(score)\n",
    "        \n",
    "    data['score'] = scores\n",
    "    \n",
    "    return data\n",
    "    \n",
    "       # if(score != 0):\n",
    "        # 0 아니면 로그찍기\n",
    "        #전체리뷰보기\n",
    "        #print(row + '...', score)\n",
    "        # 앞 10글자만 보기\n",
    "       # print(row[0:10] + '...', score)\n",
    "\n",
    "def getScoreByWord(wordname):\n",
    "    score = 0\n",
    "    for i in range(0, len(sentiwords)):\n",
    "        if wordname in sentiwords[i]['word_root']:\n",
    "            result = sentiwords[i]['polarity']\n",
    "            score = int(result)\n",
    "            # print('점수 : ' + wordname + ' : ' + str(score))\n",
    "            \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5452a5b4-9175-43f5-a3b5-ba5ebdc7242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countWords(rawData, 200).most_common(10)    \n",
    "# finalreviewscore=getScoresFromReviews(rawData, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12fb0ae9-1d51-4ac9-8413-6ced1bdc3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "getScoresFromReviews(source).to_csv('./example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18e0e2-8d17-45ee-bf7d-0fffe24433ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5fdde-c0ad-430f-a92d-67c3b7ea1f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad7d63-7c29-45be-befe-ef8039a1efad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966be32-2c6b-4faf-908b-5b062c738ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f2362-098d-4c4a-a607-ff1bcc5bc041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b944a2-a555-41bd-82e7-ace65f72b438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440a1d8-9f40-42e3-bba6-c60b3beaccf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c304b5b-2843-4003-a172-0858030c23e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2a879a-6078-49fe-aa0c-a94010b63713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n\\ndef text_cleaning(text):\\n    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 정규 표현식 처리\\n    result = hangul.sub('', text)\\n    okt = Okt()  # 형태소 추출\\n    nouns = okt.nouns(result)\\n    nouns = [x for x in nouns if len(x) > 1]  # 한글자 키워드 제거\\n    nouns = [x for x in nouns if x not in stopwords]  # 불용어 제거\\n    return nouns\\n\\nvect = CountVectorizer(tokenizer = lambda x: text_cleaning(x))\\nbow_vect = vect.fit_transform(cleaned.tolist())\\nword_list = vect.get_feature_names()\\ncount_list = bow_vect.toarray().sum(axis=0)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def text_cleaning(text):\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 정규 표현식 처리\n",
    "    result = hangul.sub('', text)\n",
    "    okt = Okt()  # 형태소 추출\n",
    "    nouns = okt.nouns(result)\n",
    "    nouns = [x for x in nouns if len(x) > 1]  # 한글자 키워드 제거\n",
    "    nouns = [x for x in nouns if x not in stopwords]  # 불용어 제거\n",
    "    return nouns\n",
    "\n",
    "vect = CountVectorizer(tokenizer = lambda x: text_cleaning(x))\n",
    "bow_vect = vect.fit_transform(cleaned.tolist())\n",
    "word_list = vect.get_feature_names()\n",
    "count_list = bow_vect.toarray().sum(axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b28735-74d9-4fe5-b67a-27d48f42ecff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb24a75-85ca-4dbe-a601-93b366acb5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c3b62-d960-4210-a613-b381695fb759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc97e9d-c121-4cc7-a84c-0ba3a8a4d17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.feature_extraction.text import TfidfTransformer\\n\\n#단어의 중요도 찾아내는 알고리즘\\ntfidf_vectorizer = TfidfTransformer()\\ntf_idf_vect = tfidf_vectorizer.fit_transform(bow_vect)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#단어의 중요도 찾아내는 알고리즘\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "tf_idf_vect = tfidf_vectorizer.fit_transform(bow_vect)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07cd18e5-a4a1-43c0-86f4-9633cc784e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tf_idf_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ca5fe-12b1-4ba2-ad61-ada601de658f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b6f38-5adb-47ff-8780-1a3ee18d8a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
